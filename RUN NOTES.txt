

Run 4:39 2/25
SMALL FILE. 
epochs = 700
lr = .0001
sgd with randomness, r = 10
Test loss 3 train 2 around epoch 93
GOT to test loss of 2, train loss of 1.2 by epoch 140

Ending: Test loss: 1.045. CLOSE. 
same stats r = 40 : couldn't get below 4.29 test. Overfitted WAY more. 
same stats lr = .0002, epochs = 1000, 
worked slightly better than the other one, got a test loss of 1.0209 to end it. 

SMALL FILE: 
Same stats r = 8, lr = .0002, epochs = 1000
BEST ONE YET: 
train loss: .4917, val loss: .888113, test loss: .88667

r = 6: BEST:
WHY IS TEST LOSS HIGHER THAN VAL???
train loss:.51512 val loss:.8069, test loss:.8329
However, maybe the reason for this is because of the low number of values we're given. 
r = 4: BEST: THIS ONE GOT BELOW THRESHOLD. LR = .0002, epochs = 1000
train loss: .5616, val loss: .748068, test loss: .77211



BIG FILE: Get a training loss of nan. 
Learning rate used on small file was way too high, used on this it'd ddecrease loss too fast then the gradient would explode. 
Changed to .00005. NOte that .00001 was too slow. 
On this dataset, each epoch gradient updates takes approximately 5.8 seconds. 
changed the loss function methods so now each is much faster, went from 7 seconds to .5. 

WIth r = 4, epochs = 100, lr = .00005, passed threshold by epoch 62. However, moving pretty slowly. This is only by updating the gradient of each row
individually. train and val losses VERY close together, suggesting almost no overfitting, probablly because way mroe examples. Also, could be 
because r is so small. 

LOSS STARTED TO GO UP AROUDN EPOCH 80 BACK OVER THE THRESHOLD???. WENT WAYYY UP. Ended at 1.10. But went down to as low as .74

Lowered epochs to 80:
Got final train loss of .735, val  loss: .7498, test loss: .7496


Changed to batch grad descent with bs 10. 
Got to the requirement by epoch 60. 80 Epochs total. 
Final train loss of .7337, val loss: .7479, test loss: .74812

Did with 100 epochs: .00005,  bs=10, r=4 batchGrad
Actually went down to as low as train: .7195 val: .7355. However, went slightly up at the end, 
final train: .7265, val: .7419, test: .7432
FIX: Make learning rate decay. 

Tried again with BS = 30, lr = .00004. 
Got trian: .737 val:.752, test:.753. Didn't run long enough. 

Trying with bs = 10, lr =.00004, epochs = 200, r = 6. 
Train: .697, val : .716, test:.71778. didn't dip at all. 

Same: r = 8
Train: .702525, val: .7288, test: .734. 
WHY IS TeST WAY HIGHER. Not enouch epochs most likely

r = 8, epochs 300:
train: .68, val: .712, test: .7119

I think 8 is too high for r, because 6 was pretty much just as effective. 


r = 6, epochs = 500, lr = .00004, bs =10:
train: .642, val: .6645, test: .664. 

r = 6 epochs 500 lr = .00004 bs = 10, added regularizer punishing above 5 or below 0, with parameter .1. 
Around epoch 100, have train .71 test .72. Epoch 172 got .7. 

Final: trainLoss: .615, val Loss = .630, testLoss = .631

param 1:
train .59 val .606 test .607


param 1: Changed to .5 on one bound: 
Got past threshold by epoch 30. 

MERGED REgularization with grad descent method, changed to cp, increased batch size to 100. 
With batch size 10(no cp) was about 1 second faster per epoch. 
With batch size 100, about same effectiveness but way faster. Not sure if bc of bs or bc of the other changes to cp. 
Got below .70 val loss at epoch 91.
Trainloss: .589, val Loss : .6065, test loss: .6043

HIGHER BATCH SIZE ONLY FASTER BC CP. 
w/ batch size 200, learning rate .00005, initially beetter, but fails to converge. 

FOR r = 6 .00005 is TOO BIG. looks like need to icnrease learning rate when loss gets too low. WRONG. DONT INCREASE LR. 
.000045 - TOO BIG